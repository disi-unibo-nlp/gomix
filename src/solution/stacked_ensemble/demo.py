import sys
from pathlib import Path
import json
from itertools import chain
from typing import List, Tuple
sys.path.append(str(Path(__file__).resolve().parents[3]))
import random
import torch
from src.utils.EmbeddedProteinsDataset import EmbeddedProteinsDataset
import argparse
from src.solution.components.naive.NaiveLearner import NaiveLearner
from src.solution.components.diamondscore.DiamondScoreLearner import DiamondScoreLearner
from src.solution.components.FC_on_embeddings.main import ALL_PROTEIN_EMBEDDINGS_DIR, \
    make_and_train_model_on as make_and_train_fc_on_embeddings_model, \
    predict_and_transform_predictions_to_dict as predict_with_nn_model_and_transform_preds_to_dict
from src.solution.stacked_ensemble.StackingMetaLearner import StackingMetaLearner
from src.solution.stacked_ensemble.Level1Dataset import Level1Dataset
from src.utils.predictions_evaluation.evaluate import evaluate_with_deepgoplus_method

DEVICE = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')


def main():
    """
    Example usage:
    python src/solution/stacked_ensemble/demo.py data/processed/task_datasets/2016/propagated_annotations/train.json data/processed/task_datasets/2016/annotations/test.json data/raw/task_datasets/2016/go.obo data/processed/task_datasets/2016/all_proteins_diamond.res
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("train_annotations_file_path", help="Path to train annotations file")
    parser.add_argument("test_annotations_file_path", help="Path to test annotations file")
    parser.add_argument("gene_ontology_file_path", help="Path to gene ontology file")
    parser.add_argument("all_proteins_diamond_scores_file_path")
    args = parser.parse_args()

    random.seed(0)
    torch.manual_seed(0)

    with open(args.train_annotations_file_path, 'r') as f:
        train_annotations = json.load(f)  # dict: prot ID -> list of GO terms

    with open(args.test_annotations_file_path, 'r') as f:
        test_annotations = json.load(f)  # dict: prot ID -> list of GO terms

    print('Train set | total proteins:', len(train_annotations), '| total GO terms:', len(set(chain.from_iterable(train_annotations.values()))))
    print('Test set | total proteins:', len(test_annotations), '| total GO terms:', len(set(chain.from_iterable(test_annotations.values()))))

    train_go_terms_vocabulary = create_go_terms_vocabulary_from_annotations(train_annotations)
    meta_learner = StackingMetaLearner(n_classes=len(train_go_terms_vocabulary))

    """
    Split training set into folds.
    The meta-learner will be trained on one of the folds, with inputs
    generated by the base models after they're trained on the other folds.
    """
    train_annotations_folds = split_dict_into_k_folds(train_annotations, k=8)

    # Train the meta-learner.
    print('\nPreparing data for training the meta-learner...')
    level1_train_base_predictions = []
    for level1_fold_idx in range(len(train_annotations_folds)):
        meta_learner_train_annotations = train_annotations_folds[level1_fold_idx]
        base_models_train_annotations = {}
        for fold_idx, fold in enumerate(train_annotations_folds):
            if fold_idx != level1_fold_idx:
                base_models_train_annotations.update(fold)

        naive_learner, diamondscore_learner, (nn_model, go_term_to_nn_output_index) = make_and_train_base_models(base_models_train_annotations, args.all_proteins_diamond_scores_file_path)
        base_models_predictions = [
            {
                prot_id: [(go_term, score) for go_term, score in naive_learner.predict().items()]
                for prot_id in meta_learner_train_annotations.keys()
            },
            {
                prot_id: [(go_term, score) for go_term, score in diamondscore_learner.predict(prot_id).items()]
                for prot_id in meta_learner_train_annotations.keys()
            },
            predict_with_nn_model_and_transform_preds_to_dict(nn_model, list(meta_learner_train_annotations.keys()), go_term_to_nn_output_index)
        ]

        for base_model_idx, pred in enumerate(base_models_predictions):
            if base_model_idx >= len(level1_train_base_predictions):
                level1_train_base_predictions.append({})
            level1_train_base_predictions[base_model_idx].update(pred)

    print(f'Level-1 train dataset generation completed. Will now train the meta-learner on {len(train_annotations)} proteins.')
    level1_train_dataset = Level1Dataset(
        go_terms_vocabulary=train_go_terms_vocabulary,
        base_predictions=level1_train_base_predictions,
        ground_truth=train_annotations,
    )
    meta_learner.fit(
        base_scores=level1_train_dataset.get_base_scores_array(),
        labels=level1_train_dataset.get_labels_array()
    )

    print('\nRe-training the base models on the whole training set...')
    naive_learner, diamondscore_learner, (nn_model, go_term_to_nn_output_index) = make_and_train_base_models(train_annotations, args.all_proteins_diamond_scores_file_path)
    print('Models re-trained successfully.')

    # ------ Test the ensemble ------ #

    print('\n------\n')
    print('Will now test the trained ensemble...')

    # Go from test set level 0 to level 1.
    print('\nPreparing level-1 test dataset...')
    test_base_predictions = [
        {
            prot_id: [(go_term, score) for go_term, score in naive_learner.predict().items()]
            for prot_id in test_annotations.keys()
        },
        {
            prot_id: [(go_term, score) for go_term, score in diamondscore_learner.predict(prot_id).items()]
            for prot_id in test_annotations.keys()
        },
        predict_with_nn_model_and_transform_preds_to_dict(nn_model, list(test_annotations.keys()), go_term_to_nn_output_index)
    ]
    level1_test_dataset = Level1Dataset(
        go_terms_vocabulary=train_go_terms_vocabulary,
        base_predictions=test_base_predictions,
    )

    final_predictions = meta_learner.predict(level1_test_dataset.get_base_scores_array())
    final_predictions = level1_test_dataset.convert_predictions_array_to_dict(final_predictions)

    print('\nTest results:')
    evaluate_with_deepgoplus_method(
        gene_ontology_file_path=args.gene_ontology_file_path,
        predictions=final_predictions,
        ground_truth=test_annotations
    )


def split_dict_into_k_folds(dictionary: dict, k: int) -> List[dict]:
    if k < 2:
        raise ValueError("Number of folds should be at least 2")

    dict_items = list(dictionary.items())
    num_items = len(dict_items)
    fold_size = num_items // k

    folds = []
    for i in range(k):
        start_idx = i * fold_size
        # If this is the last fold, include any leftover items
        if i == k - 1:
            end_idx = num_items
        else:
            end_idx = start_idx + fold_size

        fold = dict(dict_items[start_idx:end_idx])
        folds.append(fold)

    return folds


def create_go_terms_vocabulary_from_annotations(annotations: dict) -> List[str]:
    return list(set(chain.from_iterable(annotations.values())))


def make_and_train_base_models(train_annotations, all_proteins_diamond_scores_file_path):
    return (
        NaiveLearner(train_annotations),
        DiamondScoreLearner(train_annotations, all_proteins_diamond_scores_file_path),
        make_and_train_neural_fc_on_embeddings(train_annotations),
    )


def make_and_train_neural_fc_on_embeddings(train_annotations) -> Tuple[torch.nn.Module, dict]:
    print(f"\nAbout to train the NN model on {len(train_annotations)} proteins.")
    dataset = EmbeddedProteinsDataset(annotations=train_annotations, embeddings_dir=ALL_PROTEIN_EMBEDDINGS_DIR)
    return make_and_train_fc_on_embeddings_model(dataset), dataset.go_term_to_index


if __name__ == '__main__':
    main()
