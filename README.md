# Protein function prediction for CAFA challenge

## Overview

**ESM2** is a protein language model by Facebook. It can embed proteins (it first embeds amino acids, then you usually take the average between them and you get the protein embedding).

Facebook provided a script for embedding the proteins from a FASTA file, that I copied into `src/utils/embed_proteins_from_fasta`. Usage is described [here](https://github.com/facebookresearch/esm).

Example: `python src/utils/embed_proteins_from_fasta.py esm2_t33_650M_UR50D data/raw/CAFA3_training_data/uniprot_sprot_exp.fasta data/processed/CAFA3_training_data/protein_embeddings --include mean`

ESM2 model names available: https://huggingface.co/facebook/esm2_t33_650M_UR50D

### Processing annotation file

Use the script `src/utils/process_protein_annotations.py` to process an annotation file. Example usage: `python src/utils/process_protein_annotations.py data/raw/CAFA3_training_data/uniprot_sprot_exp.txt data/processed/CAFA3_training_data/protein_annotations.csv`

### Where do the data files come from?

`data/processed/CAFA3_training_data/protein_representation/STRING_v11.0_network.json` is the result of running `src/utils/prepare_STRING_network_from_raw_file.py`, which requires the following arguments:

- `protein_ids_file_path`: path to a json file containing a list of protein IDs (those that we're interested about, e.g. the prots from the CAFA dataset). This makes the resulting file lighter.
- `network_file_path`: path to the `STRING_vX_network.json` file (it should contain links that involve all the protein IDs you're interested about).
- `mapping_file_path`: uniprot_2_string file path (doesn't need to be that recent).

## Experiment 01

For now, I'm just testing the embedding of the proteins from the CAFA3 challenge, and the GO term classification with them as input, using a FC network.

### Results

**With esm2_t33_650M_UR50D embeddings:** about 20% F1-score at t=0.5 (64% precision and 11% recall).

**With esm2_t36_3B_UR50D embeddings, same model size:** about 30% F1-score at t=0.5 (65% precision and 20% recall).

**With esm2_t36_3B_UR50D embeddings, larger model + dropout:** about 35% F1-score at=0.5 (66% precision and 22% recall). F_max approximates 45%.

#### Current status

I have to try using the embeddings generated by larger versions of the ESM2 protein embedding model.

## Experiment 02: (ESM2 + PPI net) -> GCN

The idea is using PPI network information (a bit like HPODNets did).

Differently from HPODNets, we're going to initialize the graph nodes with ESM2 protein embeddings, which are then processed by a GNN.

### Training results

_Write the results here._

### Ways to improve

- Use text embeddings of protein-associated documents as input, a bit like [NetGO 2.0](https://academic.oup.com/nar/article/49/W1/W469/6285266#267025483) did (see "LR-text").
- Use [SIGN](https://arxiv.org/pdf/2004.11198.pdf) or [GraphSAINT](https://arxiv.org/abs/1907.04931) instead of the current GCN.
- Improve the current GCN with new methods such as [over-squashing prevention](https://arxiv.org/abs/2306.03589) and [half-hop](https://www.linkedin.com/posts/petarvelickovic_icml2023-activity-7090395512402534401-TGxD/?utm_source=share&utm_medium=member_desktop).
- Combine the current solutions using ensemble learning such as Stacking or Mixture of Experts. You may also add [Proteinfer](https://google-research.github.io/proteinfer/) to the ensemble.
- Another piece of information that could be added as input is the 3D structure of the proteins, coming from DBs like the Protein Data Bank (PDB). Here is a Nature paper that uses it to predict protein function.
- Add as input the 3D structure of the proteins, coming from DBs like the Protein Data Bank (PDB). [Here](https://www.nature.com/articles/s41467-021-23303-9) is a Nature paper that uses it to predict protein function.
- Include other PPI networks as input.
- Add to the input also the old protein-GO term links (with a specific edge type).

## Differential analysis

We could include in the final paper the differential analysis of various architectural decisions. Here are some of the dimensions that could be tested:
- FC on protein embeddings vs GCN with PPI edges
- ESM2 3B vs 15B
- GAT (Graph Attention Network) vs SAGEConv vs GraphSAINT vs SIGN
- different PPI nets (provided that we can get to improve the performance based on the information contained)
- different neighbor sampling thresholds
- different number of neurons or layers
- ablation of weak learners in the ensemble

## Contributions of this research

- Evaluating how informative ESM2 protein embeddings are for function prediction.
- Comparing the different types of ESM2 embeddings.
- Evaluating how informative PPI networks are for function prediction, on top of the embeddings (ablation study).
- \[?\] Evaluating how informative protein 3D structures are for function prediction (ablation study).
- Evaluating which kind of GNN is best for this task.

## Relevant papers

**CAFA:**
- [CAFA1](http://www.ncbi.nlm.nih.gov/pubmed/23353650)
- [CAFA2](http://www.ncbi.nlm.nih.gov/pubmed/27604469)
- [CAFA3](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1835-8)

**State-of-the-art for protein function prediction:**
- [NetGO 3.0](https://www.sciencedirect.com/science/article/pii/S1672022923000669)